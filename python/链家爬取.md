

# 1 创建项目

scrapy startproject lianjia

scrapy genspider lj lianjia.com

# 2 编写爬虫内容

## 1.lj.py

先从https://bj.lianjia.com/ershoufang/ 获取每个房子的链接，再进入链接爬取详细信息

```python
# -*- coding: utf-8 -*-
import scrapy

class LjSpider(scrapy.Spider):
    
    name = 'lj'
    allowed_domains = ['lianjia.com']
    # 可以看出每一页的url是由规则的，pg+n,可控制爬取页数
    start_urls = ['https://bj.lianjia.com/ershoufang/pg{}/'.format(num) for num in range(1,101)]

    提取每一页url中单条房源的url,并进行访问
    def parse(self, response):
        urls = response.xpath('//div/div[@class="title"]/a/@href').extract()
        for info_url in urls:
            # 提取了url，返回到parse_info进行解析
            yield scrapy.Request(info_url,callback=self.parse_info)
            
    def parse_info(self,response):

        # 基本属性
        info_keys = response.xpath('//div[@class="base"]/div[@class="content"]/ul/li/span/text()').extract()
        info_values = response.xpath('//div[@class="base"]/div[@class="content"]/ul/li/text()').extract()

        # 交易属性
        transaction_keys = response.xpath('//div[@class="transaction"]/div[@class="content"]/ul/li/span[1]/text()').extract()
        transaction_values = response.xpath('//div[@class="transaction"]/div[@class="content"]/ul/li/span[2]/text()').extract()

        # 总价，单价
        total_price_key = ["总价（万元）"]
        total_price_value = response.xpath('//span[@class="total"]/text()').extract()
        unit_price_key = ["单价（元/平米）"]
        unit_price_value = response.xpath('//span[@class="unitPriceValue"]/text()').extract()

        # 小区
        communityName_key = response.xpath('//div[@class="aroundInfo"]/div[@class="communityName"]/span/text()').extract()
        communityName_value = response.xpath('//div[@class="aroundInfo"]/div[@class="communityName"]/a[1]/text()').extract()

        # 所在区域
        areaName_key = response.xpath('//div[@class="aroundInfo"]/div[@class="areaName"]/span[1]/text()').extract()
        areaName_value = response.xpath('string(//div[@class="aroundInfo"]/div[@class="areaName"]/span[@class="info"])').extract()

        # 链家编号
        houseRecord_key = response.xpath('//div[@class="aroundInfo"]/div[@class="houseRecord"]/span[1]/text()').extract()
        houseRecord_value = response.xpath('//div[@class="aroundInfo"]/div[@class="houseRecord"]/span[2]/text()').extract()

        keys = houseRecord_key + total_price_key + unit_price_key + communityName_key + areaName_key + info_keys + transaction_keys
        values =houseRecord_value + total_price_value + unit_price_value + communityName_value + areaName_value + info_values + transaction_values

        infos = {}
        # 整合信息成字典键值对
        for kkey, vvalue in zip(keys, values):
            infos[kkey] = vvalue
		# 送到pipeline
        yield infos

```



## 2.pipeline

由于从lj.py传来的房源信息是字典形式，便于存到mongodb中，下面给出完整存储mongo方式

```python
import pymongo
import pymysql

class LianjiaPipeline:
    def open_spider(self,spider):
        self.client = pymongo.MongoClient()

    def process_item(self, item, spider):
        # self.client.lianjia.ershoufang.insert()
        # keys = item['keys']
        # values = item['values']

        self.client.lianjia.ershoufang.insert(item)
        return item

    def close_spider(self,spider):
        self.client.close()      
```



mysql数据库中需要有创建好的表，传过来的字典需要提取出来value才能存到mysql中，下面5，10，11需要根据需要修改，这里不写了。

```python
class MysqlPipeline(object):
    # 执行前数据库中需要有对应的表

    def open_spider(self,spider):
        self.client = pymysql.connect(host="",port="",user="",password="",db="",charset="")
        self.cursor = self.client.cursor()

    def process_item(self,item,spider):
        # 按照传过来的属性需要修改下面的sql
        args=[item["key1"],item["key2"]]
        sql='insert into 表明 values(%s,%s)'
        self.cursor.execute(sql,args)
        self.client.commit()

    def close_spider(self,spider):
        self.cursor.close()
        self.client.close()
```

## 3.setting

找到并修改setting以下内容

```python
根据您的USER_AGENT添加，可以在浏览器开发者工具network里找到

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36'

ROBOTSTXT_OBEY = False

#延迟
DOWNLOAD_DELAY = 2

# 按照pipeline里的类名命名的，如果使用mysql存，打开第二个即可
ITEM_PIPELINES = {
   'lianjia.pipelines.LianjiaPipeline': 300,
   # 'lianjia.pipelines.MysqlPipeline': 300,
}

```

## 4.启动爬虫

在项目下创建startrun.py，运行这个文件就可以启动。

```python
from scrapy.cmdline import execute

execute('scrapy crawl lj'.split())
```

